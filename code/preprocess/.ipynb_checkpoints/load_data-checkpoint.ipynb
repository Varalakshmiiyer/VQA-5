{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/sahaj/dip_project/'\n",
    "opt = {\n",
    "    'data_path' : base_path + 'data/',\n",
    "    'data_name' : 'cocotrainval',\n",
    "    'max_ques' : 14,\n",
    "    'max_ans' : None,\n",
    "    'glove_path' : base_path + 'data/glove_840B.pt',\n",
    "    'num_occurs' : 8,\n",
    "    'trainval' : True,\n",
    "}\n",
    "\n",
    "data_map_vqa = {\n",
    "    \"train\": \"v1_mscoco_train.json\",\n",
    "    \"val\": \"v1_mscoco_val.json\",\n",
    "    \"trainval\": \"v1_mscoco_trainval.json\",\n",
    "    \"testdev\": \"v1_mscoco_testdev.json\",\n",
    "    \"test\": \"v1_mscoco_test.json\",\n",
    "    \"train_comp_path\": \"v2_mscoco_train2014_complementary_pairs.json\",\n",
    "    \"val_comp_path\": \"v2_mscoco_val2014_complementary_pairs.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahaj/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Constants in the vocabulary\n",
    "UNK_WORD = \"<unk>\"\n",
    "PAD_WORD = \"<_>\"\n",
    "PAD = 0\n",
    "\n",
    "def get_top_answers(examples, occurs=0):\n",
    "    \"\"\"\n",
    "    Extract all of correct answers in the dataset. Build a set of possible answers which\n",
    "    appear more than pre-defined \"occurs\" times.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data loaded from disk.\n",
    "        occurs (int): a threshold that determine which answers are kept.\n",
    "    Return:\n",
    "        vocab_ans (list): a set of correct answers in the dataset.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for ex in examples:\n",
    "        for ans in ex[\"mc_ans\"]:\n",
    "            ans = str(ans).lower()\n",
    "            counter.update([ans])\n",
    "\n",
    "    frequent_answers = list(filter(lambda x: x[1] > occurs, counter.items()))\n",
    "    total_ans = sum(item[1] for item in counter.items())\n",
    "    total_freq_ans = sum(item[1] for item in frequent_answers)\n",
    "\n",
    "    print(\"Number of unique answers:\", len(counter))\n",
    "    print(\"Total number of answers:\", total_ans)\n",
    "    print(\"Top %i answers account for %f%%\" % (len(frequent_answers), total_freq_ans*100.0/total_ans))\n",
    "    print(\"Sample frequent answers:\")\n",
    "    print(\"\\n\".join(map(str, frequent_answers[:20])))\n",
    "\n",
    "    vocab_ans = []\n",
    "    for item in frequent_answers:\n",
    "        vocab_ans.append(item[0])\n",
    "\n",
    "    return vocab_ans\n",
    "\n",
    "\n",
    "def filter_answers(examples, ans2idx):\n",
    "    \"\"\"\n",
    "    Remove the answers that don't appear in our answer set.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data that contains all of answers in the dataset.\n",
    "        ans2idx (dict): a set of considered answers.\n",
    "    Return:\n",
    "        examples (list): the processed json data which contains only answers in the answer set.\n",
    "    \"\"\"\n",
    "    for ex in examples:\n",
    "        ex[\"ans\"] = [list(filter(lambda x: x[0] in ans2idx, answers)) for answers in ex[\"ans\"]]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Normal tokenize implementation.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        sentence (str): a setence that will be tokenized.\n",
    "    Return:\n",
    "        A list of tokens from the sentence.\n",
    "    \"\"\"\n",
    "    return [i for i in re.split(r\"([-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])\", sentence) \\\n",
    "        if i != \"\" and i != \" \" and i != \"\\n\"]\n",
    "\n",
    "\n",
    "def tokenize_mcb(sentence):\n",
    "    \"\"\"\n",
    "    MCB tokenize implementation.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        sentence (str): a setence that will be tokenized.\n",
    "    Return:\n",
    "        A list of tokens from the sentence.\n",
    "    \"\"\"\n",
    "    for i in [r\"\\?\", r\"\\!\", r\"\\'\", r\"\\\"\", r\"\\$\", r\"\\:\", r\"\\@\", r\"\\(\", r\"\\)\", r\"\\,\", r\"\\.\", r\"\\;\"]:\n",
    "        sen = re.sub(i, \"\", sen)\n",
    "\n",
    "    for i in [r\"\\-\", r\"\\/\"]:\n",
    "        sen = re.sub(i, \" \", sen)\n",
    "    q_list = re.sub(r\"\\?\", \"\", sen.lower()).split()\n",
    "    q_list = list(filter(lambda x: len(x) > 0, q_list))\n",
    "\n",
    "    return q_list\n",
    "\n",
    "\n",
    "def process_text(examples, without_ans=False, nlp=\"nltk\"):\n",
    "    \"\"\"\n",
    "    Create \"processed_ques\" and \"processed_ans\" where each question or answer is replaced\n",
    "    by an array of processed tokens using tokenizer.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data contains string of questions and answers.\n",
    "        without_ans (bool): If True, the dataset doesn't contain answers.\n",
    "        nlp (str): type of tokenize tool.\n",
    "    Return:\n",
    "        examples (list): the json data contains \"processed_ques\" and \"processed_ans\" fields.\n",
    "    \"\"\"\n",
    "    if nlp == \"nltk\":\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        import nltk\n",
    "        nltk.data.path.append(\"/home/sahaj/nltk_data\")\n",
    "        tokenizer = word_tokenize\n",
    "    elif nlp == \"mcb\":\n",
    "        tokenizer = tokenize_mcb\n",
    "    else:\n",
    "        tokenizer = tokenize\n",
    "\n",
    "    print(\"Tokenizing questions and answers...\")\n",
    "    for i, ex in enumerate(examples):\n",
    "        ex[\"processed_ques\"] = [tokenizer(str(ques).lower()) for ques in ex[\"ques\"]]\n",
    "        ex[\"processed_ans\"] = [list(map(lambda x: (tokenizer(str(x[0]).lower()), x[1]), answers)) \\\n",
    "            for answers in ex[\"ans\"]] if not without_ans else None\n",
    "\n",
    "        if i < 5:\n",
    "            print(ex[\"processed_ques\"])\n",
    "            print(ex[\"processed_ans\"]) if not without_ans else None\n",
    "\n",
    "        if (i+1) % 10000 == 0:\n",
    "            sys.stdout.write(\"processing %d/%d (%.2f%% done)    \\r\" %((i+1), len(examples), (i+1)*100.0/len(examples)))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def process_ans(ans2idx, word2idx, max_len_ans, nlp=\"nltk\"):\n",
    "    \"\"\"\n",
    "    Given the set of possible answers to predict, the function tokenize these answers and \n",
    "    replace each word with a corresponding index in the word2idx dictionary.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        ans2idx (dict): a dictionary contains answers and its index.\n",
    "        word2idx (dict): a dictionary contains words and its index.\n",
    "        max_len_ans (int): a threshold that contrains the maximum length of possible answers.\n",
    "        nlp (str): type of tokenize tool.\n",
    "    Return:\n",
    "        encoded_poss_ans (ndarray: num_ans x max_len_ans): a numpy array of possible answers \n",
    "            where each row is an answer and each column is a word.\n",
    "    \"\"\"\n",
    "    if nlp == \"nltk\":\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        import nltk\n",
    "        nltk.data.path.append(\"/home/sahaj/nltk_data\")\n",
    "        tokenizer = word_tokenize\n",
    "    elif nlp == \"mcb\":\n",
    "        tokenizer = tokenize_mcb\n",
    "    else:\n",
    "        tokenizer = tokenize\n",
    "\n",
    "    possible_answers = [[word2idx[w] if w in word2idx else word2idx[UNK_WORD] for w in tokenizer(ans)] \\\n",
    "        for ans in ans2idx.keys()]\n",
    "    encoded_poss_ans = np.zeros((len(possible_answers), max_len_ans), dtype=np.int64)\n",
    "    for i, ans in enumerate(possible_answers):\n",
    "        for j, w in enumerate(ans):\n",
    "            if j < max_len_ans:\n",
    "                encoded_poss_ans[i, j] = w\n",
    "\n",
    "    return encoded_poss_ans\n",
    "\n",
    "\n",
    "def build_glove_train(examples, gloves):\n",
    "    \"\"\"\n",
    "    Using a pre-defined vocabulary from GloVe. Convert all of word not being in the GloVe vocabulary\n",
    "    to unk word and save the new questions and answers to \"final_question\", and \"final_ans\".\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data contains list of tokens for questions and answers.\n",
    "        gloves (dict): total of GloVe words.\n",
    "    Return:\n",
    "        examples (list): the json data that filtered by GloVe vocab.\n",
    "        max_len_ans (int): maximum length of answers in dataset.\n",
    "        max_len_ques (int): maximum lenght of questions in dataset.\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    for ex in examples:\n",
    "        for ques in ex[\"processed_ques\"]:\n",
    "            counts.update(ques)\n",
    "        for answers in ex[\"processed_ans\"]:\n",
    "            for ans in answers:\n",
    "                counts.update(ans[0])\n",
    "\n",
    "    sorted_counts = sorted([(count, word) for word, count in counts.items()], reverse=True)\n",
    "    print(\"Most frequent words in the dataset:\")\n",
    "    print(\"\\n\".join(map(str, sorted_counts[:20])))\n",
    "\n",
    "    total_words = sum(counts.values())\n",
    "    print(\"Total number of words:\", total_words)\n",
    "    print(\"Number of unique words in dataset:\", len(counts))\n",
    "    print(\"Number of words in GloVe:\", len(gloves))\n",
    "\n",
    "    words_diff = frozenset(counts.keys()).difference(frozenset(gloves.keys()))\n",
    "    print(\"Number of unique words in unk: %d/%d = %.2f%%\" \n",
    "        % (len(words_diff), len(counts), len(words_diff)*100./len(counts)))\n",
    "    total_unk = sum(counts[word] for word in words_diff)\n",
    "    print(\"Total number of unk words: %d/%d = %.2f%%\" \n",
    "        % (total_unk, total_words, total_unk*100./total_words))\n",
    "\n",
    "    # Check the length distribution of questions and answers (if possible)\n",
    "    ques_lengths = Counter()\n",
    "    ans_lengths = Counter()\n",
    "\n",
    "    for ex in examples:\n",
    "        for ques in ex[\"processed_ques\"]:\n",
    "            ques_lengths.update([len(ques)])\n",
    "        for answers in ex[\"processed_ans\"]:\n",
    "            for ans in answers:\n",
    "                ans_lengths.update([len(ans[0])])\n",
    "\n",
    "    max_len_ques = max(ques_lengths.keys())\n",
    "    max_len_ans = max(ans_lengths.keys())\n",
    "\n",
    "    print(\"Max length question:\", max_len_ques)\n",
    "    print(\"Length distribution of questions (length, count):\")\n",
    "    total_questions = sum(ques_lengths.values())\n",
    "    for i in range(max_len_ques+1):\n",
    "        print(\"%2d: %10d \\t %f%%\" % (i, ques_lengths.get(i, 0), \n",
    "            ques_lengths.get(i, 0)*100./total_questions))\n",
    "\n",
    "    print(\"Max length answer:\", max_len_ans)\n",
    "    print(\"Length distribution of answers (length, count):\")\n",
    "    total_answers = sum(ans_lengths.values())\n",
    "    for i in range(max_len_ans+1):\n",
    "        print(\"%2d: %10d \\t %f%%\" % (i, ans_lengths.get(i, 0), \n",
    "            ans_lengths.get(i, 0)*100./total_answers))\n",
    "\n",
    "    for ex in examples:\n",
    "        ex[\"final_ques\"] = [[w if w in gloves else UNK_WORD for w in ques] \\\n",
    "            for ques in ex[\"processed_ques\"]]\n",
    "        ex[\"final_ans\"] = [[(list(map(lambda w: w if w in gloves else UNK_WORD, ans[0])), ans[1]) \\\n",
    "            for ans in answers] for answers in ex[\"processed_ans\"]]\n",
    "\n",
    "    return examples, max_len_ques, max_len_ans\n",
    "\n",
    "\n",
    "def filter_unk_word(examples, word2idx, without_ans=False):\n",
    "    \"\"\"\n",
    "    Given the constructed vocabulary from train or (train+val) set, convert all of words\n",
    "    that don't appear in the vocabulary to unk.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data of test set.\n",
    "        word2idx (dict): the dictionary of vocabulary constructed using train or (train+val) dataset.\n",
    "        without_ans (bool): If True, the dataset doesn't contain answers.\n",
    "    Return:\n",
    "        examples (list): the updated json data where words not being in the vocabulary are set to unk.\n",
    "    \"\"\"\n",
    "    for ex in examples:\n",
    "        ex[\"final_ques\"] = [[w if w in word2idx else UNK_WORD for w in ques]\n",
    "            for ques in ex[\"processed_ques\"]]\n",
    "        ex[\"final_ans\"] = [[(list(map(lambda w: w if w in word2idx else UNK_WORD, ans[0])), ans[1]) \\\n",
    "            for ans in answers] for answers in ex[\"processed_ans\"]] if not without_ans else None\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def encode_ans(examples, ans2idx):\n",
    "    \"\"\"\n",
    "    Convert answers for each question to its index.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data contains answers for each question.\n",
    "        ans2idx (dict): dictionary of answers and its indices.\n",
    "    Return:\n",
    "        examples (list): the updated data where answers are replaced by its index.\n",
    "    \"\"\"\n",
    "    for ex in examples:\n",
    "        ex[\"ans_id\"] = [list(map(lambda x: (ans2idx[x[0]], x[1]), answers)) for answers in ex[\"ans\"]]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def encode_VQA(examples, max_len_ques, num_ans, word2idx, without_ans=False):\n",
    "    \"\"\"\n",
    "    Using the processed json data to create numpy array which contains information of\n",
    "    questions, images, and index of correct answers in set of possible answers.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the process json data.\n",
    "        max_len_ques (int): the maximum length of question allowed.\n",
    "        num_ans (int): number of possible answers in the pre-defined set.\n",
    "        word2idx (dict): dictionary of vocabulary.\n",
    "        without_ans (bool): If True, the dataset doesn't contain answers.\n",
    "    Return:\n",
    "        img_idx (ndarray: num_sample): index of images.\n",
    "        ques_array (ndarray: num_ques x max_len_ques): question data in numpy array.\n",
    "        txt_start_idx (ndarray: num_sample): start index of questions of the same image.\n",
    "        txt_end_idx (ndarray: num_sample): end index of questions of the same image.\n",
    "        ans_idx (ndarray: num_ques x num_poss_ans): ground truth scores of possible answers\n",
    "        ques_idx (ndarray: num_ques): question index data corresponds to each question.\n",
    "    \"\"\"\n",
    "    N = len(examples)\n",
    "    M = sum(len(ex[\"final_ques\"]) for ex in examples)\n",
    "\n",
    "    ques_array = np.zeros((M, max_len_ques), dtype=np.int64)\n",
    "    img_idx = np.zeros(N, dtype=np.int64)\n",
    "    txt_start_idx = np.zeros(N, dtype=np.int64)\n",
    "    txt_end_idx = np.zeros(N, dtype=np.int64)\n",
    "    ques_idx = np.zeros(M, dtype=np.int64)\n",
    "    ans_idx = np.zeros((M, num_ans), dtype=np.float32) if not without_ans else None\n",
    "\n",
    "    txt_counter = 0\n",
    "    counter = 0\n",
    "\n",
    "    for i, ex in enumerate(examples):\n",
    "        n = len(ex[\"final_ques\"])\n",
    "        assert n > 0, \"Some images has no questions\"\n",
    "\n",
    "        img_idx[i] = ex[\"id\"]\n",
    "        for j, ques in enumerate(ex[\"final_ques\"]):\n",
    "            ques_idx[txt_counter] = ex[\"ques_id\"][j]\n",
    "\n",
    "            if not without_ans:\n",
    "                for ans in ex[\"ans_id\"][j]:\n",
    "                    ans_idx[txt_counter, ans[0]] = ans[1]\n",
    "\n",
    "            assert len(ques) > 0, \"Question has no words!\"\n",
    "            for k, w in enumerate(ques):\n",
    "                if k < max_len_ques:\n",
    "                    ques_array[txt_counter, k] = word2idx[w]\n",
    "\n",
    "            txt_counter += 1\n",
    "\n",
    "        txt_start_idx[i] = counter\n",
    "        txt_end_idx[i] = counter + n - 1\n",
    "        counter += n\n",
    "\n",
    "    assert txt_counter == M, \"Number of questions doesn't match!\"\n",
    "    print(\"Encoded array of questions:\", str(ques_array.shape))\n",
    "\n",
    "    return (img_idx, ques_array, txt_start_idx, txt_end_idx, ans_idx, ques_idx)\n",
    "\n",
    "\n",
    "def process_dataset(dataset, num_occurs, glove_path, max_ques, max_ans):\n",
    "    \"\"\"\n",
    "    Process the loaded json file into a dataset which can be fed into a neural network.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        dataset (list): the json data loaded from disk.\n",
    "        num_occurs (int): a threshold that determine which answers are kept.\n",
    "        glove_path (str): path points to the file storing GloVe vectors.\n",
    "        max_ques (int): maximum length of question to be processed.\n",
    "        max_ans (int): maximum length of answer to be processed.\n",
    "    Return:\n",
    "        ans2idx (dict): indices to possible answers.\n",
    "        idx2ans (dict): possible answers to its indices.\n",
    "        word2idx (dict): dictionary of vocabulary from words to indices.\n",
    "        idx2word (dict): dictionary of vocabulary from indices to words.\n",
    "        dataset (list): processed dataset which contains encoded information.\n",
    "        max_len_ques (int): maximum length of questions in dataset if max_ques is not set.\n",
    "        poss_answers (ndarray: num_ques x num_poss_ans): a set of answers to predict.\n",
    "    \"\"\"\n",
    "    top_answers = get_top_answers(dataset, num_occurs)\n",
    "    num_ans = len(top_answers)\n",
    "    ans2idx = {}\n",
    "    for idx, ans in enumerate(top_answers):\n",
    "        ans2idx[ans] = idx\n",
    "    idx2ans = top_answers\n",
    "\n",
    "    dataset = filter_answers(dataset, ans2idx)\n",
    "    dataset = process_text(dataset)\n",
    "\n",
    "    assert glove_path is not None, \"Couldn't find GloVe file!\"\n",
    "    gloves = torch.load(glove_path)\n",
    "    print(\"gloves type:\", type(gloves))\n",
    "    dataset, max_len_ques, max_len_ans = build_glove_train(dataset, gloves[\"word2idx\"])\n",
    "    idx2word = gloves[\"idx2word\"]\n",
    "    word2idx = gloves[\"word2idx\"]\n",
    "\n",
    "    max_len_ques = max_ques if max_ques is not None else max_len_ques\n",
    "    max_len_ans = max_ans if max_ans is not None else max_len_ans\n",
    "\n",
    "    dataset = encode_ans(dataset, ans2idx)\n",
    "    poss_answers = process_ans(ans2idx, word2idx, max_len_ans)\n",
    "\n",
    "    return ans2idx, idx2ans, word2idx, idx2word, dataset, max_len_ques, poss_answers\n",
    "\n",
    "\n",
    "def create_dataset(data_path, data_name, data_type, dataset, max_len_ques, \n",
    "                   poss_ans, word2idx, comp=None):\n",
    "    \"\"\"\n",
    "    Create a h5py file and store all of numpy arrays of the dataset into that file.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        data_path (str): path to the directory for storing the dataset file.\n",
    "        data_name (str): name of the dataset.\n",
    "        data_type (str): \"train\", \"val\", \"trainval\", \"test\", or \"testdev\".\n",
    "        dataset list): the json data.\n",
    "        max_len_ques (int): the maximum length of questions.\n",
    "        poss_ans (ndarray: num_ques x num_poss_ans): a set of answers to predict.\n",
    "        word2idx (dict): dictionary of vocabulary.\n",
    "        comp (list): the json data containing complementary pairs.\n",
    "    Return:\n",
    "        Create the data set in h5py file.\n",
    "    \"\"\"\n",
    "    num_ans = poss_ans.shape[0]\n",
    "    without_ans = True if data_type in [\"testdev\", \"test\"] else False\n",
    "    (img_idx, ques_array, txt_start_idx, txt_end_idx, ans_idx, ques_idx) = \\\n",
    "        encode_VQA(dataset, max_len_ques, num_ans, word2idx, without_ans)\n",
    "\n",
    "    if comp is not None:\n",
    "        comp_idx = np.zeros((len(comp), 2), dtype=np.int64)\n",
    "        for i, pair in enumerate(comp):\n",
    "            comp_idx[i, 0] = np.argwhere(ques_idx == pair[0])[0, 0]\n",
    "            comp_idx[i, 1] = np.argwhere(ques_idx == pair[1])[0, 0]\n",
    "\n",
    "    file = h5py.File(os.path.join(data_path, \"%s_%s.h5\" % (data_name, data_type)), \"w\")\n",
    "    file.create_dataset(\"img_idx\", dtype=np.int64, data=img_idx)\n",
    "    file.create_dataset(\"questions\", dtype=np.int64, data=ques_array)\n",
    "    file.create_dataset(\"txt_start_idx\", dtype=np.int64, data=txt_start_idx)\n",
    "    file.create_dataset(\"txt_end_idx\", dtype=np.int64, data=txt_end_idx)\n",
    "    file.create_dataset(\"ques_idx\", dtype=np.int64, data=ques_idx)\n",
    "    file.create_dataset(\"ans_pool\", dtype=np.int64, data=poss_ans)\n",
    "    file.create_dataset(\"ans_idx\", dtype=np.float32, data=ans_idx) if not without_ans else None\n",
    "    file.create_dataset(\"comp_idx\", dtype=np.int64, data=comp_idx) if comp is not None else None\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Constants in the vocabulary\n",
    "UNK_WORD = \"<unk>\"\n",
    "PAD_WORD = \"<_>\"\n",
    "PAD = 0\n",
    "\n",
    "def get_top_answers(examples, occurs=0):\n",
    "    \"\"\"\n",
    "    Extract all of correct answers in the dataset. Build a set of possible answers which\n",
    "    appear more than pre-defined \"occurs\" times.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data loaded from disk.\n",
    "        occurs (int): a threshold that determine which answers are kept.\n",
    "    Return:\n",
    "        vocab_ans (list): a set of correct answers in the dataset.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for ex in examples:\n",
    "        for ans in ex[\"mc_ans\"]:\n",
    "            ans = str(ans).lower()\n",
    "            counter.update([ans])\n",
    "\n",
    "    frequent_answers = list(filter(lambda x: x[1] > occurs, counter.items()))\n",
    "    total_ans = sum(item[1] for item in counter.items())\n",
    "    total_freq_ans = sum(item[1] for item in frequent_answers)\n",
    "\n",
    "    print(\"Number of unique answers:\", len(counter))\n",
    "    print(\"Total number of answers:\", total_ans)\n",
    "    print(\"Top %i answers account for %f%%\" % (len(frequent_answers), total_freq_ans*100.0/total_ans))\n",
    "    print(\"Sample frequent answers:\")\n",
    "    print(\"\\n\".join(map(str, frequent_answers[:20])))\n",
    "\n",
    "    vocab_ans = []\n",
    "    for item in frequent_answers:\n",
    "        vocab_ans.append(item[0])\n",
    "\n",
    "    return vocab_ans\n",
    "\n",
    "\n",
    "def filter_answers(examples, ans2idx):\n",
    "    \"\"\"\n",
    "    Remove the answers that don't appear in our answer set.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data that contains all of answers in the dataset.\n",
    "        ans2idx (dict): a set of considered answers.\n",
    "    Return:\n",
    "        examples (list): the processed json data which contains only answers in the answer set.\n",
    "    \"\"\"\n",
    "    for ex in examples:\n",
    "        ex[\"ans\"] = [list(filter(lambda x: x[0] in ans2idx, answers)) for answers in ex[\"ans\"]]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Normal tokenize implementation.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        sentence (str): a setence that will be tokenized.\n",
    "    Return:\n",
    "        A list of tokens from the sentence.\n",
    "    \"\"\"\n",
    "    return [i for i in re.split(r\"([-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])\", sentence) \\\n",
    "        if i != \"\" and i != \" \" and i != \"\\n\"]\n",
    "\n",
    "\n",
    "def tokenize_mcb(sentence):\n",
    "    \"\"\"\n",
    "    MCB tokenize implementation.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        sentence (str): a setence that will be tokenized.\n",
    "    Return:\n",
    "        A list of tokens from the sentence.\n",
    "    \"\"\"\n",
    "    for i in [r\"\\?\", r\"\\!\", r\"\\'\", r\"\\\"\", r\"\\$\", r\"\\:\", r\"\\@\", r\"\\(\", r\"\\)\", r\"\\,\", r\"\\.\", r\"\\;\"]:\n",
    "        sen = re.sub(i, \"\", sen)\n",
    "\n",
    "    for i in [r\"\\-\", r\"\\/\"]:\n",
    "        sen = re.sub(i, \" \", sen)\n",
    "    q_list = re.sub(r\"\\?\", \"\", sen.lower()).split()\n",
    "    q_list = list(filter(lambda x: len(x) > 0, q_list))\n",
    "\n",
    "    return q_list\n",
    "\n",
    "\n",
    "def process_text(examples, without_ans=False, nlp=\"nltk\"):\n",
    "    \"\"\"\n",
    "    Create \"processed_ques\" and \"processed_ans\" where each question or answer is replaced\n",
    "    by an array of processed tokens using tokenizer.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data contains string of questions and answers.\n",
    "        without_ans (bool): If True, the dataset doesn't contain answers.\n",
    "        nlp (str): type of tokenize tool.\n",
    "    Return:\n",
    "        examples (list): the json data contains \"processed_ques\" and \"processed_ans\" fields.\n",
    "    \"\"\"\n",
    "    if nlp == \"nltk\":\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        import nltk\n",
    "        nltk.data.path.append(\"/home/sahaj/nltk_data\")\n",
    "        tokenizer = word_tokenize\n",
    "    elif nlp == \"mcb\":\n",
    "        tokenizer = tokenize_mcb\n",
    "    else:\n",
    "        tokenizer = tokenize\n",
    "\n",
    "    print(\"Tokenizing questions and answers...\")\n",
    "    for i, ex in enumerate(examples):\n",
    "        ex[\"processed_ques\"] = [tokenizer(str(ques).lower()) for ques in ex[\"ques\"]]\n",
    "        ex[\"processed_ans\"] = [list(map(lambda x: (tokenizer(str(x[0]).lower()), x[1]), answers)) \\\n",
    "            for answers in ex[\"ans\"]] if not without_ans else None\n",
    "\n",
    "        if i < 5:\n",
    "            print(ex[\"processed_ques\"])\n",
    "            print(ex[\"processed_ans\"]) if not without_ans else None\n",
    "\n",
    "        if (i+1) % 10000 == 0:\n",
    "            sys.stdout.write(\"processing %d/%d (%.2f%% done)    \\r\" %((i+1), len(examples), (i+1)*100.0/len(examples)))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def process_ans(ans2idx, word2idx, max_len_ans, nlp=\"nltk\"):\n",
    "    \"\"\"\n",
    "    Given the set of possible answers to predict, the function tokenize these answers and \n",
    "    replace each word with a corresponding index in the word2idx dictionary.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        ans2idx (dict): a dictionary contains answers and its index.\n",
    "        word2idx (dict): a dictionary contains words and its index.\n",
    "        max_len_ans (int): a threshold that contrains the maximum length of possible answers.\n",
    "        nlp (str): type of tokenize tool.\n",
    "    Return:\n",
    "        encoded_poss_ans (ndarray: num_ans x max_len_ans): a numpy array of possible answers \n",
    "            where each row is an answer and each column is a word.\n",
    "    \"\"\"\n",
    "    if nlp == \"nltk\":\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        import nltk\n",
    "        nltk.data.path.append(\"/home/sahaj/nltk_data\")\n",
    "        tokenizer = word_tokenize\n",
    "    elif nlp == \"mcb\":\n",
    "        tokenizer = tokenize_mcb\n",
    "    else:\n",
    "        tokenizer = tokenize\n",
    "\n",
    "    possible_answers = [[word2idx[w] if w in word2idx else word2idx[UNK_WORD] for w in tokenizer(ans)] \\\n",
    "        for ans in ans2idx.keys()]\n",
    "    encoded_poss_ans = np.zeros((len(possible_answers), max_len_ans), dtype=np.int64)\n",
    "    for i, ans in enumerate(possible_answers):\n",
    "        for j, w in enumerate(ans):\n",
    "            if j < max_len_ans:\n",
    "                encoded_poss_ans[i, j] = w\n",
    "\n",
    "    return encoded_poss_ans\n",
    "\n",
    "\n",
    "def build_glove_train(examples, gloves):\n",
    "    \"\"\"\n",
    "    Using a pre-defined vocabulary from GloVe. Convert all of word not being in the GloVe vocabulary\n",
    "    to unk word and save the new questions and answers to \"final_question\", and \"final_ans\".\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data contains list of tokens for questions and answers.\n",
    "        gloves (dict): total of GloVe words.\n",
    "    Return:\n",
    "        examples (list): the json data that filtered by GloVe vocab.\n",
    "        max_len_ans (int): maximum length of answers in dataset.\n",
    "        max_len_ques (int): maximum lenght of questions in dataset.\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    for ex in examples:\n",
    "        for ques in ex[\"processed_ques\"]:\n",
    "            counts.update(ques)\n",
    "        for answers in ex[\"processed_ans\"]:\n",
    "            for ans in answers:\n",
    "                counts.update(ans[0])\n",
    "\n",
    "    sorted_counts = sorted([(count, word) for word, count in counts.items()], reverse=True)\n",
    "    print(\"Most frequent words in the dataset:\")\n",
    "    print(\"\\n\".join(map(str, sorted_counts[:20])))\n",
    "\n",
    "    total_words = sum(counts.values())\n",
    "    print(\"Total number of words:\", total_words)\n",
    "    print(\"Number of unique words in dataset:\", len(counts))\n",
    "    print(\"Number of words in GloVe:\", len(gloves))\n",
    "\n",
    "    words_diff = frozenset(counts.keys()).difference(frozenset(gloves.keys()))\n",
    "    print(\"Number of unique words in unk: %d/%d = %.2f%%\" \n",
    "        % (len(words_diff), len(counts), len(words_diff)*100./len(counts)))\n",
    "    total_unk = sum(counts[word] for word in words_diff)\n",
    "    print(\"Total number of unk words: %d/%d = %.2f%%\" \n",
    "        % (total_unk, total_words, total_unk*100./total_words))\n",
    "\n",
    "    # Check the length distribution of questions and answers (if possible)\n",
    "    ques_lengths = Counter()\n",
    "    ans_lengths = Counter()\n",
    "\n",
    "    for ex in examples:\n",
    "        for ques in ex[\"processed_ques\"]:\n",
    "            ques_lengths.update([len(ques)])\n",
    "        for answers in ex[\"processed_ans\"]:\n",
    "            for ans in answers:\n",
    "                ans_lengths.update([len(ans[0])])\n",
    "\n",
    "    max_len_ques = max(ques_lengths.keys())\n",
    "    max_len_ans = max(ans_lengths.keys())\n",
    "\n",
    "    print(\"Max length question:\", max_len_ques)\n",
    "    print(\"Length distribution of questions (length, count):\")\n",
    "    total_questions = sum(ques_lengths.values())\n",
    "    for i in range(max_len_ques+1):\n",
    "        print(\"%2d: %10d \\t %f%%\" % (i, ques_lengths.get(i, 0), \n",
    "            ques_lengths.get(i, 0)*100./total_questions))\n",
    "\n",
    "    print(\"Max length answer:\", max_len_ans)\n",
    "    print(\"Length distribution of answers (length, count):\")\n",
    "    total_answers = sum(ans_lengths.values())\n",
    "    for i in range(max_len_ans+1):\n",
    "        print(\"%2d: %10d \\t %f%%\" % (i, ans_lengths.get(i, 0), \n",
    "            ans_lengths.get(i, 0)*100./total_answers))\n",
    "\n",
    "    for ex in examples:\n",
    "        ex[\"final_ques\"] = [[w if w in gloves else UNK_WORD for w in ques] \\\n",
    "            for ques in ex[\"processed_ques\"]]\n",
    "        ex[\"final_ans\"] = [[(list(map(lambda w: w if w in gloves else UNK_WORD, ans[0])), ans[1]) \\\n",
    "            for ans in answers] for answers in ex[\"processed_ans\"]]\n",
    "\n",
    "    return examples, max_len_ques, max_len_ans\n",
    "\n",
    "\n",
    "def filter_unk_word(examples, word2idx, without_ans=False):\n",
    "    \"\"\"\n",
    "    Given the constructed vocabulary from train or (train+val) set, convert all of words\n",
    "    that don't appear in the vocabulary to unk.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data of test set.\n",
    "        word2idx (dict): the dictionary of vocabulary constructed using train or (train+val) dataset.\n",
    "        without_ans (bool): If True, the dataset doesn't contain answers.\n",
    "    Return:\n",
    "        examples (list): the updated json data where words not being in the vocabulary are set to unk.\n",
    "    \"\"\"\n",
    "    for ex in examples:\n",
    "        ex[\"final_ques\"] = [[w if w in word2idx else UNK_WORD for w in ques]\n",
    "            for ques in ex[\"processed_ques\"]]\n",
    "        ex[\"final_ans\"] = [[(list(map(lambda w: w if w in word2idx else UNK_WORD, ans[0])), ans[1]) \\\n",
    "            for ans in answers] for answers in ex[\"processed_ans\"]] if not without_ans else None\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def encode_ans(examples, ans2idx):\n",
    "    \"\"\"\n",
    "    Convert answers for each question to its index.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the json data contains answers for each question.\n",
    "        ans2idx (dict): dictionary of answers and its indices.\n",
    "    Return:\n",
    "        examples (list): the updated data where answers are replaced by its index.\n",
    "    \"\"\"\n",
    "    for ex in examples:\n",
    "        ex[\"ans_id\"] = [list(map(lambda x: (ans2idx[x[0]], x[1]), answers)) for answers in ex[\"ans\"]]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def encode_VQA(examples, max_len_ques, num_ans, word2idx, without_ans=False):\n",
    "    \"\"\"\n",
    "    Using the processed json data to create numpy array which contains information of\n",
    "    questions, images, and index of correct answers in set of possible answers.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        examples (list): the process json data.\n",
    "        max_len_ques (int): the maximum length of question allowed.\n",
    "        num_ans (int): number of possible answers in the pre-defined set.\n",
    "        word2idx (dict): dictionary of vocabulary.\n",
    "        without_ans (bool): If True, the dataset doesn't contain answers.\n",
    "    Return:\n",
    "        img_idx (ndarray: num_sample): index of images.\n",
    "        ques_array (ndarray: num_ques x max_len_ques): question data in numpy array.\n",
    "        txt_start_idx (ndarray: num_sample): start index of questions of the same image.\n",
    "        txt_end_idx (ndarray: num_sample): end index of questions of the same image.\n",
    "        ans_idx (ndarray: num_ques x num_poss_ans): ground truth scores of possible answers\n",
    "        ques_idx (ndarray: num_ques): question index data corresponds to each question.\n",
    "    \"\"\"\n",
    "    N = len(examples)\n",
    "    M = sum(len(ex[\"final_ques\"]) for ex in examples)\n",
    "\n",
    "    ques_array = np.zeros((M, max_len_ques), dtype=np.int64)\n",
    "    img_idx = np.zeros(N, dtype=np.int64)\n",
    "    txt_start_idx = np.zeros(N, dtype=np.int64)\n",
    "    txt_end_idx = np.zeros(N, dtype=np.int64)\n",
    "    ques_idx = np.zeros(M, dtype=np.int64)\n",
    "    ans_idx = np.zeros((M, num_ans), dtype=np.float32) if not without_ans else None\n",
    "\n",
    "    txt_counter = 0\n",
    "    counter = 0\n",
    "\n",
    "    for i, ex in enumerate(examples):\n",
    "        n = len(ex[\"final_ques\"])\n",
    "        assert n > 0, \"Some images has no questions\"\n",
    "\n",
    "        img_idx[i] = ex[\"id\"]\n",
    "        for j, ques in enumerate(ex[\"final_ques\"]):\n",
    "            ques_idx[txt_counter] = ex[\"ques_id\"][j]\n",
    "\n",
    "            if not without_ans:\n",
    "                for ans in ex[\"ans_id\"][j]:\n",
    "                    ans_idx[txt_counter, ans[0]] = ans[1]\n",
    "\n",
    "            assert len(ques) > 0, \"Question has no words!\"\n",
    "            for k, w in enumerate(ques):\n",
    "                if k < max_len_ques:\n",
    "                    ques_array[txt_counter, k] = word2idx[w]\n",
    "\n",
    "            txt_counter += 1\n",
    "\n",
    "        txt_start_idx[i] = counter\n",
    "        txt_end_idx[i] = counter + n - 1\n",
    "        counter += n\n",
    "\n",
    "    assert txt_counter == M, \"Number of questions doesn't match!\"\n",
    "    print(\"Encoded array of questions:\", str(ques_array.shape))\n",
    "\n",
    "    return (img_idx, ques_array, txt_start_idx, txt_end_idx, ans_idx, ques_idx)\n",
    "\n",
    "\n",
    "def process_dataset(dataset, num_occurs, glove_path, max_ques, max_ans):\n",
    "    \"\"\"\n",
    "    Process the loaded json file into a dataset which can be fed into a neural network.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        dataset (list): the json data loaded from disk.\n",
    "        num_occurs (int): a threshold that determine which answers are kept.\n",
    "        glove_path (str): path points to the file storing GloVe vectors.\n",
    "        max_ques (int): maximum length of question to be processed.\n",
    "        max_ans (int): maximum length of answer to be processed.\n",
    "    Return:\n",
    "        ans2idx (dict): indices to possible answers.\n",
    "        idx2ans (dict): possible answers to its indices.\n",
    "        word2idx (dict): dictionary of vocabulary from words to indices.\n",
    "        idx2word (dict): dictionary of vocabulary from indices to words.\n",
    "        dataset (list): processed dataset which contains encoded information.\n",
    "        max_len_ques (int): maximum length of questions in dataset if max_ques is not set.\n",
    "        poss_answers (ndarray: num_ques x num_poss_ans): a set of answers to predict.\n",
    "    \"\"\"\n",
    "    top_answers = get_top_answers(dataset, num_occurs)\n",
    "    num_ans = len(top_answers)\n",
    "    ans2idx = {}\n",
    "    for idx, ans in enumerate(top_answers):\n",
    "        ans2idx[ans] = idx\n",
    "    idx2ans = top_answers\n",
    "\n",
    "    dataset = filter_answers(dataset, ans2idx)\n",
    "    dataset = process_text(dataset)\n",
    "\n",
    "    assert glove_path is not None, \"Couldn't find GloVe file!\"\n",
    "    gloves = torch.load(glove_path)\n",
    "    print(\"gloves type:\", type(gloves))\n",
    "    dataset, max_len_ques, max_len_ans = build_glove_train(dataset, gloves[\"word2idx\"])\n",
    "    idx2word = gloves[\"idx2word\"]\n",
    "    word2idx = gloves[\"word2idx\"]\n",
    "\n",
    "    max_len_ques = max_ques if max_ques is not None else max_len_ques\n",
    "    max_len_ans = max_ans if max_ans is not None else max_len_ans\n",
    "\n",
    "    dataset = encode_ans(dataset, ans2idx)\n",
    "    poss_answers = process_ans(ans2idx, word2idx, max_len_ans)\n",
    "\n",
    "    return ans2idx, idx2ans, word2idx, idx2word, dataset, max_len_ques, poss_answers\n",
    "\n",
    "\n",
    "def create_dataset(data_path, data_name, data_type, dataset, max_len_ques, \n",
    "                   poss_ans, word2idx, comp=None):\n",
    "    \"\"\"\n",
    "    Create a h5py file and store all of numpy arrays of the dataset into that file.\n",
    "    --------------------\n",
    "    Arguments:\n",
    "        data_path (str): path to the directory for storing the dataset file.\n",
    "        data_name (str): name of the dataset.\n",
    "        data_type (str): \"train\", \"val\", \"trainval\", \"test\", or \"testdev\".\n",
    "        dataset list): the json data.\n",
    "        max_len_ques (int): the maximum length of questions.\n",
    "        poss_ans (ndarray: num_ques x num_poss_ans): a set of answers to predict.\n",
    "        word2idx (dict): dictionary of vocabulary.\n",
    "        comp (list): the json data containing complementary pairs.\n",
    "    Return:\n",
    "        Create the data set in h5py file.\n",
    "    \"\"\"\n",
    "    num_ans = poss_ans.shape[0]\n",
    "    without_ans = True if data_type in [\"testdev\", \"test\"] else False\n",
    "    (img_idx, ques_array, txt_start_idx, txt_end_idx, ans_idx, ques_idx) = \\\n",
    "        encode_VQA(dataset, max_len_ques, num_ans, word2idx, without_ans)\n",
    "\n",
    "    if comp is not None:\n",
    "        print(len(comp))\n",
    "        comp_idx = np.zeros((len(comp), 2), dtype=np.int64)\n",
    "        for i, pair in enumerate(comp):\n",
    "            try:\n",
    "                comp_idx[i, 0] = np.argwhere(ques_idx == pair[0])[0, 0] \n",
    "            except:\n",
    "                #comp_idx[i, 0] = None\n",
    "                print(\"Exception\")\n",
    "            try:\n",
    "                comp_idx[i, 1] = np.argwhere(ques_idx == pair[1])[0, 0]\n",
    "            except:\n",
    "                print(\"Exception\")\n",
    "                #comp_idx[i, 1] = None\n",
    "                \n",
    "    file = h5py.File(os.path.join(data_path, \"%s_%s.h5\" % (data_name, data_type)), \"w\")\n",
    "    file.create_dataset(\"img_idx\", dtype=np.int64, data=img_idx)\n",
    "    file.create_dataset(\"questions\", dtype=np.int64, data=ques_array)\n",
    "    file.create_dataset(\"txt_start_idx\", dtype=np.int64, data=txt_start_idx)\n",
    "    file.create_dataset(\"txt_end_idx\", dtype=np.int64, data=txt_end_idx)\n",
    "    file.create_dataset(\"ques_idx\", dtype=np.int64, data=ques_idx)\n",
    "    file.create_dataset(\"ans_pool\", dtype=np.int64, data=poss_ans)\n",
    "    file.create_dataset(\"ans_idx\", dtype=np.float32, data=ans_idx) if not without_ans else None\n",
    "    file.create_dataset(\"comp_idx\", dtype=np.int64, data=comp_idx) if comp is not None else None\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process train dataset...\n",
      "Number of unique answers: 17140\n",
      "Total number of answers: 248349\n",
      "Top 1607 answers account for 89.505092%\n",
      "Sample frequent answers:\n",
      "('curved', 11)\n",
      "('yes', 58729)\n",
      "('1', 4714)\n",
      "('no', 36920)\n",
      "('white', 4460)\n",
      "('trees', 213)\n",
      "('red', 3618)\n",
      "('vegetables', 74)\n",
      "('picture', 90)\n",
      "('on bed', 39)\n",
      "('gold', 81)\n",
      "('right', 1018)\n",
      "('stool', 17)\n",
      "('kitten', 14)\n",
      "('mouse', 52)\n",
      "('cheese', 164)\n",
      "('brown', 1662)\n",
      "('black', 2269)\n",
      "('calm', 43)\n",
      "('weeds', 14)\n",
      "Tokenizing questions and answers...\n",
      "[['what', 'shape', 'is', 'the', 'bench', 'seat', '?']]\n",
      "[[(['oval'], 0.3), (['curved'], 1), (['banana'], 0.3)]]\n",
      "[['is', 'there', 'a', 'shadow', '?']]\n",
      "[[(['yes'], 1)]]\n",
      "[['is', 'this', 'one', 'bench', 'or', 'multiple', 'benches', '?']]\n",
      "[[(['1'], 1)]]\n",
      "[['is', 'this', 'a', 'modern', 'train', '?']]\n",
      "[[(['no'], 1)]]\n",
      "[['what', 'color', 'is', 'the', 'stripe', 'on', 'the', 'train', '?']]\n",
      "[[(['white'], 1)]]\n",
      "gloves type: <class 'dict'>6.64% done)    \n",
      "Most frequent words in the dataset:\n",
      "(248445, '?')\n",
      "(182071, 'the')\n",
      "(156533, 'is')\n",
      "(103130, 'what')\n",
      "(79031, 'yes')\n",
      "(60179, 'no')\n",
      "(57095, 'are')\n",
      "(49037, 'this')\n",
      "(39066, 'on')\n",
      "(38983, 'in')\n",
      "(32530, 'a')\n",
      "(32494, 'of')\n",
      "(28322, 'how')\n",
      "(26753, 'many')\n",
      "(25259, 'color')\n",
      "(20439, 'there')\n",
      "(15335, 'man')\n",
      "(13527, 'white')\n",
      "(13081, '2')\n",
      "(12655, 'does')\n",
      "Total number of words: 2272167\n",
      "Number of unique words in dataset: 13824\n",
      "Number of words in GloVe: 2196018\n",
      "Number of unique words in unk: 241/13824 = 1.74%\n",
      "Total number of unk words: 251/2272167 = 0.01%\n",
      "Max length question: 25\n",
      "Length distribution of questions (length, count):\n",
      " 0:          0 \t 0.000000%\n",
      " 1:          0 \t 0.000000%\n",
      " 2:          0 \t 0.000000%\n",
      " 3:         11 \t 0.004429%\n",
      " 4:       6692 \t 2.694595%\n",
      " 5:      33886 \t 13.644508%\n",
      " 6:      63770 \t 25.677575%\n",
      " 7:      49095 \t 19.768552%\n",
      " 8:      40910 \t 16.472786%\n",
      " 9:      25576 \t 10.298411%\n",
      "10:      12710 \t 5.117798%\n",
      "11:       6882 \t 2.771100%\n",
      "12:       3950 \t 1.590504%\n",
      "13:       2159 \t 0.869341%\n",
      "14:       1111 \t 0.447354%\n",
      "15:        664 \t 0.267366%\n",
      "16:        400 \t 0.161064%\n",
      "17:        209 \t 0.084156%\n",
      "18:        146 \t 0.058788%\n",
      "19:         84 \t 0.033823%\n",
      "20:         53 \t 0.021341%\n",
      "21:         27 \t 0.010872%\n",
      "22:          6 \t 0.002416%\n",
      "23:          7 \t 0.002819%\n",
      "24:          0 \t 0.000000%\n",
      "25:          1 \t 0.000403%\n",
      "Max length answer: 6\n",
      "Length distribution of answers (length, count):\n",
      " 0:          0 \t 0.000000%\n",
      " 1:     408441 \t 93.889734%\n",
      " 2:      16863 \t 3.876356%\n",
      " 3:       8933 \t 2.053459%\n",
      " 4:        669 \t 0.153785%\n",
      " 5:         56 \t 0.012873%\n",
      " 6:         60 \t 0.013792%\n",
      "Encoded array of questions: (248349, 14)\n",
      "Process val dataset...\n",
      "Tokenizing questions and answers...\n",
      "[['what', 'is', 'the', 'table', 'made', 'of', '?']]\n",
      "[[(['wood'], 1)]]\n",
      "[['is', 'the', 'food', 'napping', 'on', 'the', 'table', '?']]\n",
      "[[(['no'], 1), (['yes'], 0.6)]]\n",
      "[['what', 'has', 'been', 'upcycled', 'to', 'make', 'lights', '?']]\n",
      "[[(['kettle'], 0.3)]]\n",
      "[['is', 'this', 'an', 'spanish', 'town', '?']]\n",
      "[[(['yes'], 1), (['no'], 1)]]\n",
      "[['are', 'there', 'shadows', 'on', 'the', 'sidewalk', '?']]\n",
      "[[(['yes'], 1)]]\n",
      "Encoded array of questions: (121512, 14)  \n",
      "Process testdev dataset...\n",
      "Tokenizing questions and answers...\n",
      "[['are', 'the', 'dogs', 'tied', '?']]\n",
      "[['is', 'this', 'a', 'car', 'show', '?']]\n",
      "[['is', 'there', 'a', 'lady', 'sitting', 'inside', 'the', 'red', 'truck', '?']]\n",
      "[['where', 'is', 'the', 'giraffe', 'headed', '?']]\n",
      "[['what', 'kind', 'of', 'animal', 'is', 'pictured', '?']]\n",
      "Encoded array of questions: (60864, 14) \n",
      "Process test dataset...\n",
      "Tokenizing questions and answers...\n",
      "[['are', 'the', 'dogs', 'tied', '?']]\n",
      "[['is', 'this', 'a', 'car', 'show', '?']]\n",
      "[['is', 'there', 'a', 'lady', 'sitting', 'inside', 'the', 'red', 'truck', '?']]\n",
      "[['is', 'the', 'man', 'surfing', '?']]\n",
      "[['what', 'color', 'is', 'the', 'man', \"'s\", 'swimsuit', '?']]\n",
      "Encoded array of questions: (244302, 14)  \n",
      "Saving information file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create dataset for \"train\", \"val\", \"test\", and \"testdev\" or \"trainval\", \"test\", and \"testdev\".\n",
    "\"\"\"\n",
    "if not opt['trainval']:\n",
    "    print(\"Process train dataset...\")\n",
    "    train_set = json.load(open(os.path.join(opt['data_path'], data_map_vqa[\"train\"]), \"r\"))\n",
    "    #comp_train = json.load(open(os.path.join(opt['data_path'], data_map_vqa[\"train_comp_path\"]), \"r\"))\n",
    "    comp_train = None\n",
    "    \n",
    "    ans2idx, idx2ans, word2idx, idx2word, train_set, max_len_ques, poss_answers = \\\n",
    "        process_dataset(train_set, opt['num_occurs'], opt['glove_path'], opt['max_ques'], opt['max_ans'])\n",
    "\n",
    "    create_dataset(opt['data_path'], opt['data_name'], \"train\", train_set, max_len_ques, \n",
    "        poss_answers, word2idx, comp=comp_train)\n",
    "\n",
    "    print(\"Process val dataset...\")\n",
    "    val_set = json.load(open(os.path.join(opt['data_path'], data_map_vqa[\"val\"]), \"r\"))\n",
    "    #comp_val = json.load(open(os.path.join(opt['data_path'], data_map_vqa[\"val_comp_path\"]), \"r\"))\n",
    "    comp_val = None\n",
    "    \n",
    "    val_set = filter_answers(val_set, ans2idx)\n",
    "    val_set = process_text(val_set)\n",
    "    val_set = filter_unk_word(val_set, word2idx)\n",
    "    val_set = encode_ans(val_set, ans2idx)\n",
    "\n",
    "    create_dataset(opt['data_path'], opt['data_name'], \"val\", val_set, max_len_ques,\n",
    "        poss_answers, word2idx, comp=comp_val)\n",
    "else:\n",
    "    print(\"Process trainval dataset...\")\n",
    "    trainval_set = json.load(open(os.path.join(opt['data_path'], data_map_vqa[\"trainval\"]), \"r\"))\n",
    "    #comp_trainval = json.load(open(os.path.join(opt['data_path'], data_map_vqa[\"train_comp_path\"]), \"r\"))\n",
    "    #comp_trainval.extend(json.load(open(os.path.join(opt['data_path'], data_map_vqa[\"val_comp_path\"]), \"r\")))\n",
    "    comp_trainval = None\n",
    "    \n",
    "    ans2idx, idx2ans, word2idx, idx2word, trainval_set, max_len_ques, poss_answers = \\\n",
    "        process_dataset(trainval_set, opt['num_occurs'], opt['glove_path'], opt['max_ques'], opt['max_ans'])\n",
    "\n",
    "    create_dataset(opt['data_path'], opt['data_name'], \"trainval\", trainval_set, max_len_ques,\n",
    "        poss_answers, word2idx, comp=comp_trainval)\n",
    "\n",
    "print(\"Process testdev dataset...\")\n",
    "testdev_set = json.load(open(os.path.join(opt['data_path'], data_map_vqa[\"testdev\"]), \"r\"))\n",
    "\n",
    "testdev_set = process_text(testdev_set, without_ans=True)\n",
    "testdev_set = filter_unk_word(testdev_set, word2idx, without_ans=True)\n",
    "\n",
    "create_dataset(opt['data_path'], opt['data_name'], \"testdev\", testdev_set, max_len_ques,\n",
    "    poss_answers, word2idx)\n",
    "\n",
    "print(\"Process test dataset...\")\n",
    "test_set = json.load(open(os.path.join(opt['data_path'], data_map_vqa[\"test\"]), \"r\"))\n",
    "\n",
    "test_set = process_text(test_set, without_ans=True)\n",
    "test_set = filter_unk_word(test_set, word2idx, without_ans=True)\n",
    "\n",
    "create_dataset(opt['data_path'], opt['data_name'], \"test\", test_set, max_len_ques,\n",
    "    poss_answers, word2idx)\n",
    "\n",
    "print(\"Saving information file...\")\n",
    "info = {\n",
    "    \"ans2idx\": ans2idx,\n",
    "    \"idx2ans\": idx2ans,\n",
    "    \"word2idx\": word2idx,\n",
    "    \"idx2word\": idx2word,\n",
    "}\n",
    "torch.save(info, os.path.join(opt['data_path'], \"%s_info.pt\" % (opt['data_name'])))\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
